---
title: "P8105_HW3_hx2306"
author: "HuijunXiao"
date: "10/18/2021"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(knitr)
library(patchwork)
```

### Problem 1  
* Step 1: Load data from P8105.dataset.   
```{r}
library(p8105.datasets)
data("instacart")
```
* Step 2: Short description of the dataset.   
```{r}
# How many aisles are there, and which aisles are the most items ordered from?
aisles <-
  instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```
     
The `instacart` dataset contains `r ncol(instacart)` variables, which are `r colnames(instacart)`, and `r nrow(instacart)` rows of observations. For aisles, there are `r nrow(aisles)` kinds of aisles, and the most items are ordered from `fresh vegetables` with n = 150609. The second is `fresh fruits` with n = 150473. The third is `packaged vegetables fruits` with n = 78493. There are `r instacart %>% distinct(product_name) %>%count` products in total. 
    
* Step 3: Make a plot.      
```{r}
# Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items 
aisles_plot <-
  aisles %>%
  filter(n > 10000) %>%
  mutate(aisle = fct_reorder(aisle,desc(n))) %>%   #reorder in ggplot
  ggplot(aes(x =aisle, y = n)) + 
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Aisle") +
  ylab("Count") + 
  ggtitle("The number of items ordered in each aisle with more than 10000 items")

aisles_plot
```
        
* Step 4: Make a table.    
```{r}
# Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
aisle_item <-
  c("baking ingredients", "dog food care", "packaged vegetables fruits")

aisle_table <- 
  instacart %>%
  filter(aisle %in% aisle_item) %>%
  select(product_name, aisle) %>%
  group_by(aisle) %>% 
  count(product_name) %>%
  arrange(desc(n), .by_group = TRUE) %>%
  mutate(rank_item = order(n, decreasing = TRUE)) %>%     #rank for three most popular items in each aisle 
  filter(rank_item <= 3) %>%
  relocate(rank_item,aisle, product_name,n) %>%
  rename(Aisle = aisle,
         `Product Name` = product_name,
         Count = n,
         Rank = rank_item) 

knitr::kable(aisle_table) 
  
```
      
* Step 5: Make a table.  
```{r}
# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
aisle_item2 <-
  c("Pink Lady Apples","Coffee Ice Cream")

aisle_table2 <- 
  instacart %>%
  filter(product_name %in% aisle_item2) %>%
  select(product_name,order_dow,order_hour_of_day) %>%
  group_by(product_name,order_dow) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
        names_from = order_dow,
        values_from = mean_hour) %>%
  rename(`Product Name` = product_name)

knitr::kable(aisle_table2,digits = 3) 
```
    
### Problem 2    
* Step 1: Data cleaning.    
```{r}
data("brfss_smart2010")

brfss_smart2010 <-
  brfss_smart2010 %>%                                #format the data to use appropriate variable names
  rename(State = Locationabbr,
         Location = Locationdesc,
         Data_Value = Data_value, 
         Confidence_Limit_Low = Confidence_limit_Low,
         Confidence_Limit_High = Confidence_limit_High,
         Display_Order = Display_order,
         Data_Value_Unit = Data_value_unit,
         Data_Value_Type = Data_value_type, 
         Data_Source = DataSource,
         Class_Id = ClassId,
         Topic_Id = TopicId,
         Location_ID = LocationID,
         Question_ID = QuestionID,
         RESP_ID = RESPID,
         Location_Geo = GeoLocation) %>%
  filter(Topic == "Overall Health") %>%                #focus on the “Overall Health” topic
  filter(Response == "Excellent" |
           Response == "Fair" |
           Response == "Good" |
           Response == "Very good"|
           Response == "Poor") %>%                     #include only responses from “Excellent” to “Poor”
  mutate( 
    Response = factor(
      Response,
      levels = c("Poor","Fair","Good","Very good","Excellent")))  #organize responses as a factor taking levels ordered from “Poor” to “Excellent”

```
      
* Step 2: Which states were observed at 7 or more locations    
```{r}
#In 2002, which states were observed at 7 or more locations? What about in 2010?
state_2002 <- 
  brfss_smart2010 %>%
  filter(Year == "2002")%>%           #year 2002
  count(State,Location) %>%
  group_by(State) %>%
  tally() %>%                         #count state locations
  filter( n >= 7)                     #7 or more locations

state_2010 <- 
  brfss_smart2010 %>%
  filter(Year == "2010")%>%           #year 2010
  count(State,Location) %>%
  group_by(State) %>%
  tally() %>%                         #count state locations
  filter( n >= 7)                     #7 or more locations
```
    
In 2002, there are only `r nrow(state_2002)` states were observed at 7 or more locations, which are `r state_2002$State`. In 2010, there are only `r nrow(state_2010)` states were observed at 7 or more locations, which are `r state_2010$State`.    
     
* Step 3: Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state.     
```{r}
#dataset
exc_response <-
  brfss_smart2010 %>%
  filter(Response == "Excellent") %>%
  select(Year,State,Data_Value)%>%
  group_by(Year, State) %>%
  summarise(Mean_Data_Value = mean(Data_Value)) %>%
  ggplot(aes(x = Year,                                        #spaghetti plots
           y = Mean_Data_Value, 
           group = State, 
           color = State)) +
  geom_line() + 
  ggtitle("Average Data Value Within A State Across Years") +
  ylab("Data Value Average") +
  labs(color = "State") + 
  theme(legend.position = "none")

exc_response
```
        
Based on the plot, the average data value of these states are slightly decreasing in general. CT has relatively the highest average data value line across years from 2002 to 2010, and WV has the lowest average data value line across years from 2002 to 2010.   
     
* Step 4: Make a two-panel plot.     
```{r}
NY_state_plot <-
  brfss_smart2010 %>%
  filter(State == "NY",
         Year == "2006" | Year == "2010") %>%
  ggplot(aes(x = Response , y = Data_Value)) +
  geom_boxplot() +
  facet_grid(.~Year)                                       #combine two plots

NY_state_plot
```
      
For the response of `Poor`, the data value interquartile range of 2010 is lager than that of 2006, but has lower median. Same situation happens for the response of `Fair`. The interquartile range of `Good` response in 2010 is smaller than that in 2006, but 2010 has higher median. For both `Very good` and `Excellent` responses, 2010 has higher data value, although the interquartile ranges are similar.     
      
### Problem 3   
* Step 1: Load, tidy, and otherwise wrangle the data.     
```{r}
accel_data <-
  read.csv("./dataset/accel_data.csv") %>% 
  janitor::clean_names() %>%                                    #has useful variable names
  pivot_longer(activity_1:activity_1440,                        #pivot longer 
               names_to = "Time(minute)",
               values_to = "Activity_Counts") %>%
  mutate(Weekday_vs_Weekend =                                   #weekday vs weekend variable
           case_when(
             day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday") ~ "Weekday",
             day %in% c("Saturday","Sunday") ~ "Weekend")) %>%
  rename(Week = week,
         Day_ID = day_id,
         Day = day) %>%
  mutate(`Time(minute)` = str_remove(`Time(minute)`,"activity_"),
         `Time(minute)` = as.numeric(`Time(minute)`))
```
    
This dataset contains `r ncol(accel_data)` variables, which are `r colnames(accel_data)`, and `r nrow(accel_data)` rows of observations. It includes five weeks of accelerometer data collected on a 63-year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF).    
    
* Step 2: Traditional analyses of accelerometer data focus on the total activity over the day.    
```{r}
# aggregate across minutes to create a total activity variable for each day
total_act <-
  accel_data %>%
  group_by(Week, Day) %>%
  summarise(sum(Activity_Counts)) %>%
  rename(Total_Activity = `sum(Activity_Counts)`) %>%
  pivot_wider(
    names_from = Day,
    values_from = Total_Activity) %>%
  relocate(Week,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday,Sunday)

# create a table showing these totals
knitr::kable(total_act) 
```
      
The trends are not apparent based on the table. In the first week, the total activity increases over the week (from Monday to Sunday) in general. The same situation happens in the second week. However, during Week 3, Monday has the highest total activity. We also cannot conclude that weekdays usually have lower total activity than weekends, since, for Week 4 and 5 weekends, their total activity values are all below 300,000.     
     
* Step 3: The inspection activity over the course of the day.     
```{r}
# Make a single-panel plot that shows the 24-hour activity time courses for each day
day_plot <-
  ggplot(accel_data, 
         aes(x = `Time(minute)`, y = Activity_Counts, group = Day_ID, color = Day)) + 
  # use color to indicate day of the week
  geom_line(alpha = .2) +
  geom_smooth(aes(group = Day), se = FALSE) +
  ggtitle("24-hour activity time courses for each day") +
  xlab("Minute") +
  ylab("Activity Counts Each Minute") 

day_plot
```
       
Based on the plot, we can tell that this subject had higher total activity counts at the beginning or the end (close to) of the day. On average, Friday and Wednesday have higher total activity for each minute at the beginning of the day, and Sunday and Saturday have higher total activity for each minute at the end of the day. In the middle period of the day, the subject's total activity is relatively lower.     
